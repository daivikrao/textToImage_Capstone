# textToImage_Capstone
We have trained a model on the flower dataset having captions with Glove Embedding containing 1.29M vocab and compared the results with other model having similar architecture but using Glove Embedding with 120K vocab.
We found the results obtained were better with the model using Glove Embedding containing 1.29M vocab than the one with 120K vocab.
